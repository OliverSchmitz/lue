#pragma once
#include "lue/framework/algorithm/policy.hpp"
#include "lue/framework/core/component.hpp"
#include "lue/framework/io/configure.hpp"
#include "lue/framework/io/dataset.hpp"
#include "lue/framework/io/util.hpp"
#include "lue/data_model/hl/util.hpp"


/*!
    @file

    Thi—ï header contains the implementation of the various to_lue function overloads.

    The goal is to return an array as soon as possible, allowing the caller to create more work, while
    the write-tasks start the writing.
*/


namespace lue {
    namespace detail {

        template<typename Policies, typename Partition, typename CreateHyperslab, typename Property>
        void write_partition(
            [[maybe_unused]] Policies const& policies,
            Partition const& partition,
            CreateHyperslab create_hyperslab,
            data_model::ID const object_id,
            Property& property)
        {
            // Open value. Configure for use of parallel I/O if necessary.
            hdf5::Dataset::TransferPropertyList transfer_property_list{};

#if LUE_FRAMEWORK_WITH_PARALLEL_IO
            transfer_property_list.set_transfer_mode(H5FD_MPIO_INDEPENDENT);
#endif

            auto& value{property.value()};
            auto array{value[object_id]};

            using Element = ElementT<Partition>;

            auto partition_ptr{detail::ready_component_ptr(partition)};
            auto& partition_server{*partition_ptr};
            Element* buffer{partition_server.data().data()};

            array.write(create_hyperslab(partition_server), transfer_property_list, buffer);
        }


        template<typename Policies, typename Partitions>
        auto write_partitions_constant(
            Policies const& policies,
            hdf5::Offset const& array_hyperslab_start,  // Only needed to offset block written to array
            Partitions const& partitions,
            std::string const& array_pathname,
            data_model::ID const object_id) -> hpx::future<void>
        {
            auto const [dataset_pathname, phenomenon_name, property_set_name, property_name] =
                parse_array_pathname(array_pathname);

            return hpx::when_any(partitions)
                .then(
                    [policies,
                     array_hyperslab_start,
                     dataset_pathname,
                     phenomenon_name,
                     property_set_name,
                     property_name,
                     object_id](auto when_any_result_f)
                    {
                        // Open the dataset once the first partition is ready to be written
                        auto dataset = open_dataset(dataset_pathname, H5F_ACC_RDWR);

                        // Now that dataset is open, write the ready partition to it

                        // Open phenomenon
                        auto& phenomenon{dataset.phenomena()[phenomenon_name]};

                        // Open property-set
                        auto& property_set{phenomenon.property_sets()[property_set_name]};

                        // Open property
                        lue_hpx_assert(property_set.properties().contains(property_name));
                        lue_hpx_assert(
                            property_set.properties().shape_per_object(property_name) ==
                            data_model::ShapePerObject::different);
                        lue_hpx_assert(
                            property_set.properties().value_variability(property_name) ==
                            data_model::ValueVariability::constant);
                        using Properties = data_model::different_shape::Properties;
                        auto& property{property_set.properties().collection<Properties>()[property_name]};

                        using Partition = typename Partitions::value_type;
                        using PartitionServer = Partition::Server;

                        auto create_hyperslab =
                            [array_hyperslab_start](PartitionServer const& partition_server)
                        { return hyperslab(array_hyperslab_start, partition_server); };

                        auto [partition_idx, partitions] = when_any_result_f.get();
                        write_partition(
                            policies, partitions[partition_idx], create_hyperslab, object_id, property);
                        partitions.erase(partitions.begin() + partition_idx);

                        // Write any other ready partition, until all partitions are written, one after the
                        // other
                        while (!partitions.empty())
                        {
                            auto [partition_idx, partitions_] = hpx::when_any(partitions).get();

                            write_partition(
                                policies, partitions_[partition_idx], create_hyperslab, object_id, property);

                            partitions_.erase(partitions_.begin() + partition_idx);

                            partitions = std::move(partitions_);
                        }
                    });
        }


        template<typename Policies, typename Partitions>
        auto write_partitions_variable(
            Policies const& policies,
            hdf5::Offset const& array_hyperslab_start,  // Only needed to offset block written to array
            Partitions const& partitions,
            std::string const& array_pathname,
            data_model::ID const object_id,
            Index const time_step_idx) -> hpx::future<void>
        {
            auto const [dataset_pathname, phenomenon_name, property_set_name, property_name] =
                parse_array_pathname(array_pathname);

            return hpx::when_any(partitions)
                .then(
                    [policies,
                     array_hyperslab_start,
                     dataset_pathname,
                     phenomenon_name,
                     property_set_name,
                     property_name,
                     object_id,
                     time_step_idx](auto&& when_any_result_f)
                    {
                        // Open the dataset once the first partition is ready to be written
                        auto dataset = open_dataset(dataset_pathname, H5F_ACC_RDWR);

                        // Now that dataset is open, write the ready partition to it

                        // Open phenomenon
                        auto& phenomenon{dataset.phenomena()[phenomenon_name]};

                        // Open property-set
                        auto& property_set{phenomenon.property_sets()[property_set_name]};

                        // Open property
                        lue_hpx_assert(property_set.properties().contains(property_name));
                        lue_hpx_assert(
                            property_set.properties().shape_per_object(property_name) ==
                            data_model::ShapePerObject::different);
                        lue_hpx_assert(
                            property_set.properties().value_variability(property_name) ==
                            data_model::ValueVariability::variable);
                        lue_hpx_assert(
                            property_set.properties().shape_variability(property_name) ==
                            data_model::ShapeVariability::constant);
                        using Properties = data_model::different_shape::constant_shape::Properties;
                        auto const& property{
                            property_set.properties().collection<Properties>()[property_name]};

                        using Partition = typename Partitions::value_type;
                        using PartitionServer = Partition::Server;

                        auto create_hyperslab =
                            [array_hyperslab_start, time_step_idx](PartitionServer const& partition_server)
                        { return hyperslab(array_hyperslab_start, partition_server, 0, time_step_idx); };

                        auto [partition_idx, partitions] = when_any_result_f.get();
                        write_partition(
                            policies, partitions[partition_idx], create_hyperslab, object_id, property);
                        partitions.erase(partitions.begin() + partition_idx);

                        // Write any other ready partition, until all partition are written, one after the
                        // other
                        while (!partitions.empty())
                        {
                            auto [partition_idx, partitions_] = hpx::when_any(partitions).get();

                            write_partition(
                                policies, partitions_[partition_idx], create_hyperslab, object_id, property);

                            partitions_.erase(partitions_.begin() + partition_idx);

                            partitions = std::move(partitions_);
                        }
                    });
        }


        template<typename Policies, typename Partitions>
        struct WritePartitionsConstantAction:
            hpx::actions::make_action<
                decltype(&write_partitions_constant<Policies, Partitions>),
                &write_partitions_constant<Policies, Partitions>,
                WritePartitionsConstantAction<Policies, Partitions>>::type
        {
        };


        template<typename Policies, typename Partitions>
        struct WritePartitionsVariableAction:
            hpx::actions::make_action<
                decltype(&write_partitions_variable<Policies, Partitions>),
                &write_partitions_variable<Policies, Partitions>,
                WritePartitionsVariableAction<Policies, Partitions>>::type
        {
        };

    }  // namespace detail


    namespace policy::to_lue {

        template<typename InputElement>
        using DefaultPolicies =
            policy::DefaultPolicies<AllValuesWithinDomain<>, OutputElements<>, InputElements<InputElement>>;

        template<typename InputElement>
        using DefaultValuePolicies = policy::
            DefaultValuePolicies<AllValuesWithinDomain<>, OutputElements<>, InputElements<InputElement>>;

    }  // namespace policy::to_lue


    /*!
        @brief      Write an array to an array in a LUE dataset
        @tparam     Policies Policies type
        @tparam     Rank Rank of the array
        @param      policies Policies to use
        @param      array Array to write
        @param      array_pathname Pathname of the property to write to, formatted as
                    `<dataset_pathname>/<phenomenon_name>/<property_set_name>/<property_name>`
        @param      object_id ID of object whose property value to write
        @return     A future which becomes ready once the writing is done
    */
    template<typename Policies, Rank rank>
    [[nodiscard]] auto to_lue(
        Policies const& policies,
        PartitionedArray<policy::InputElementT<Policies>, rank> const& array,
        std::string const& array_pathname,
        data_model::ID const object_id) -> hpx::future<void>
    {
        using Element = policy::InputElementT<Policies>;
        using Array = PartitionedArray<Element, rank>;
        using Partition = PartitionT<Array>;

        auto const partition_idxs_by_locality{detail::partition_idxs_by_locality(array)};
        lue_hpx_assert(partition_idxs_by_locality.size() <= hpx::find_all_localities().size());

        std::vector<hpx::shared_future<void>> localities_finished{};
        localities_finished.reserve(partition_idxs_by_locality.size());

        using Action = detail::WritePartitionsConstantAction<Policies, std::vector<Partition>>;
        Action action{};

        for (auto const& [locality, partition_idxs] : partition_idxs_by_locality)
        {
            // Copy current partitions from input array to a new collection
            std::vector<Partition> partitions{};
            partitions.resize(partition_idxs.size());

            for (std::size_t idx = 0; auto const partition_idx : partition_idxs)
            {
                partitions[idx++] = array.partitions()[partition_idx];
            }

            hpx::shared_future<void> previous_locality_finished =
                localities_finished.empty() ? hpx::make_ready_future() : localities_finished.back();

            // Spawn a task that writes the current partitions to the dataset. This returns a future which
            // becomes ready once the partitions have been written.
#if !LUE_FRAMEWORK_WITH_PARALLEL_IO
            localities_finished.push_back(previous_locality_finished.then(
                [locality,
                 action,
                 policies,
                 array_hyperslab = detail::shape_to_hyperslab(array.shape()),
                 partitions = std::move(partitions),
                 array_pathname,
                 object_id]([[maybe_unused]] auto const& previous_locality_finished)
                {
                    return action(
                        locality,
                        std::move(policies),
                        array_hyperslab.start(),
                        std::move(partitions),
                        std::move(array_pathname),
                        object_id);
                }));
#else
            localities_finished.push_back(
                [locality,
                 action,
                 policies,
                 array_hyperslab = detail::shape_to_hyperslab(array.shape()),
                 partitions = std::move(partitions),
                 array_pathname,
                 object_id]()
                {
                    return action(
                        locality,
                        std::move(policies),
                        array_hyperslab.start(),
                        std::move(partitions),
                        std::move(array_pathname),
                        object_id);
                }());
#endif
        }

        return hpx::when_all(localities_finished.begin(), localities_finished.end());
    }


    /*!
        @overload

        Default policies will be used.
    */
    template<typename Element, Rank rank>
    [[nodiscard]] auto to_lue(
        PartitionedArray<Element, rank> const& array,
        std::string const& array_pathname,
        data_model::ID const object_id) -> hpx::future<void>
    {
        using Policies = policy::to_lue::DefaultPolicies<Element>;

        return to_lue(Policies{}, array, array_pathname, object_id);
    }


    /*!
        @brief      Write an array to an array in a LUE dataset
        @tparam     Policies Policies type
        @tparam     Rank Rank of the array
        @param      policies Policies to use
        @param      array Array to write
        @param      array_pathname Pathname of the property to write to, formatted as
                    `<dataset_pathname>/<phenomenon_name>/<property_set_name>/<property_name>`
        @param      object_id ID of object whose property value to write
        @param      time_step_idx Index of time step to write
        @return     A future which becomes ready once the writing is done
    */
    template<typename Policies, Rank rank>
    [[nodiscard]] auto to_lue(
        Policies const& policies,
        PartitionedArray<policy::InputElementT<Policies>, rank> const& array,
        std::string const& array_pathname,
        data_model::ID const object_id,
        Index const time_step_idx) -> hpx::future<void>
    {
        using Element = policy::InputElementT<Policies>;
        using Array = PartitionedArray<Element, rank>;
        using Partition = PartitionT<Array>;

        // Group partitions by locality
        auto const partition_idxs_by_locality{detail::partition_idxs_by_locality(array)};
        lue_hpx_assert(partition_idxs_by_locality.size() <= hpx::find_all_localities().size());

        std::vector<hpx::shared_future<void>> localities_finished{};
        localities_finished.reserve(partition_idxs_by_locality.size());

        using Action = detail::WritePartitionsVariableAction<Policies, std::vector<Partition>>;
        Action action{};

        // Only a single locality can open the file for writing at the same time. Write partitions per
        // locality, serialized.

        // Iterate over all grouped partitions
        for (auto const& [locality, partition_idxs] : partition_idxs_by_locality)
        {
            // Copy current partitions from input array to a new collection
            std::vector<Partition> partitions{};
            partitions.resize(partition_idxs.size());

            for (std::size_t idx = 0; auto const partition_idx : partition_idxs)
            {
                partitions[idx++] = array.partitions()[partition_idx];
            }

            hpx::shared_future<void> previous_locality_finished =
                localities_finished.empty() ? hpx::make_ready_future() : localities_finished.back();

            // Spawn a task that writes the current partitions to the dataset. This returns a future which
            // becomes ready once the partitions have been written.
#if !LUE_FRAMEWORK_WITH_PARALLEL_IO
            localities_finished.push_back(previous_locality_finished.then(
                [locality,
                 action,
                 policies,
                 array_hyperslab = detail::shape_to_hyperslab(array.shape()),
                 partitions = std::move(partitions),
                 array_pathname,
                 object_id,
                 time_step_idx]([[maybe_unused]] auto const& previous_locality_finished)
                {
                    return action(
                        locality,
                        std::move(policies),
                        array_hyperslab.start(),
                        std::move(partitions),
                        std::move(array_pathname),
                        object_id,
                        time_step_idx);
                }));
#else
            localities_finished.push_back(
                [locality,
                 action,
                 policies,
                 array_hyperslab = detail::shape_to_hyperslab(array.shape()),
                 partitions = std::move(partitions),
                 array_pathname,
                 object_id,
                 time_step_idx]()
                {
                    return action(
                        locality,
                        std::move(policies),
                        array_hyperslab.start(),
                        std::move(partitions),
                        std::move(array_pathname),
                        object_id,
                        time_step_idx);
                }());
#endif
        }

        return hpx::when_all(localities_finished.begin(), localities_finished.end());
    }


    /*!
        @overload

        Default policies will be used.
    */
    template<typename Element, Rank rank>
    [[nodiscard]] auto to_lue(
        PartitionedArray<Element, rank> const& array,
        std::string const& array_pathname,
        data_model::ID const object_id,
        Index const time_step_idx) -> hpx::future<void>
    {
        using Policies = policy::to_lue::DefaultPolicies<Element>;

        return to_lue(Policies{}, array, array_pathname, object_id, time_step_idx);
    }

}  // namespace lue
